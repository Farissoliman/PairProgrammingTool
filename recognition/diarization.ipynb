{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diart'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcontextlib\u001b[39;00m \u001b[39mimport\u001b[39;00m contextmanager\n\u001b[0;32m----> 7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdiart\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperators\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdops\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrich\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'diart'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import diart.operators as dops\n",
    "import numpy as np\n",
    "import rich\n",
    "import rx.operators as ops\n",
    "import whisper_timestamped as whisper\n",
    "from diart import OnlineSpeakerDiarization, PipelineConfig\n",
    "from diart.sources import MicrophoneAudioSource\n",
    "from pyannote.core import Annotation, SlidingWindowFeature, SlidingWindow, Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(chunks, collar=0.05):\n",
    "    \"\"\"\n",
    "    Concatenate predictions and audio\n",
    "    given a list of `(diarization, waveform)` pairs\n",
    "    and merge contiguous single-speaker regions\n",
    "    with pauses shorter than `collar` seconds.\n",
    "    \"\"\"\n",
    "    first_annotation = chunks[0][0]\n",
    "    first_waveform = chunks[0][1]\n",
    "    annotation = Annotation(uri=first_annotation.uri)\n",
    "    data = []\n",
    "    for ann, wav in chunks:\n",
    "        annotation.update(ann)\n",
    "        data.append(wav.data)\n",
    "    annotation = annotation.support(collar)\n",
    "    window = SlidingWindow(\n",
    "        first_waveform.sliding_window.duration,\n",
    "        first_waveform.sliding_window.step,\n",
    "        first_waveform.sliding_window.start,\n",
    "    )\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    return annotation, SlidingWindowFeature(data, window)\n",
    "\n",
    "\n",
    "def colorize_transcription(transcription):\n",
    "    \"\"\"\n",
    "    Unify a speaker-aware transcription represented as\n",
    "    a list of `(speaker: int, text: str)` pairs\n",
    "    into a single text colored by speakers.\n",
    "    \"\"\"\n",
    "    colors = 2 * [\n",
    "        \"bright_red\", \"bright_blue\", \"bright_green\", \"orange3\", \"deep_pink1\",\n",
    "        \"yellow2\", \"magenta\", \"cyan\", \"bright_magenta\", \"dodger_blue2\"\n",
    "    ]\n",
    "    result = []\n",
    "    for speaker, text in transcription:\n",
    "        if speaker == -1:\n",
    "            # No speakerfound for this text, use default terminal color\n",
    "            result.append(text)\n",
    "        else:\n",
    "            result.append(f\"[{colors[speaker]}]{text}\")\n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    # Auxiliary function to suppress Whisper logs (it is quite verbose)\n",
    "    # All credit goes to: https://thesmithfam.org/blog/2012/10/25/temporarily-suppress-console-output-in-python/\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "class WhisperTranscriber:\n",
    "    def __init__(self, model=\"small\", device=None):\n",
    "        self.model = whisper.load_model(model, device=device)\n",
    "        self._buffer = \"\"\n",
    "\n",
    "    def transcribe(self, waveform):\n",
    "        \"\"\"Transcribe audio using Whisper\"\"\"\n",
    "        # Pad/trim audio to fit 30 seconds as required by Whisper\n",
    "        audio = waveform.data.astype(\"float32\").reshape(-1)\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "        # Transcribe the given audio while suppressing logs\n",
    "        with suppress_stdout():\n",
    "            transcription = whisper.transcribe(\n",
    "                self.model,\n",
    "                audio,\n",
    "                # We use past transcriptions to condition the model\n",
    "                initial_prompt=self._buffer,\n",
    "                verbose=True  # to avoid progress bar\n",
    "            )\n",
    "\n",
    "        return transcription\n",
    "\n",
    "    def identify_speakers(self, transcription, diarization, time_shift):\n",
    "        \"\"\"Iterate over transcription segments to assign speakers\"\"\"\n",
    "        speaker_captions = []\n",
    "        for segment in transcription[\"segments\"]:\n",
    "\n",
    "            # Crop diarization to the segment timestamps\n",
    "            start = time_shift + segment[\"words\"][0][\"start\"]\n",
    "            end = time_shift + segment[\"words\"][-1][\"end\"]\n",
    "            dia = diarization.crop(Segment(start, end))\n",
    "\n",
    "            # Assign a speaker to the segment based on diarization\n",
    "            speakers = dia.labels()\n",
    "            num_speakers = len(speakers)\n",
    "            if num_speakers == 0:\n",
    "                # No speakers were detected\n",
    "                caption = (-1, segment[\"text\"])\n",
    "            elif num_speakers == 1:\n",
    "                # Only one speaker is active in this segment\n",
    "                spk_id = int(speakers[0].split(\"speaker\")[1])\n",
    "                caption = (spk_id, segment[\"text\"])\n",
    "            else:\n",
    "                # Multiple speakers, select the one that speaks the most\n",
    "                max_speaker = int(np.argmax([\n",
    "                    dia.label_duration(spk) for spk in speakers\n",
    "                ]))\n",
    "                caption = (max_speaker, segment[\"text\"])\n",
    "            speaker_captions.append(caption)\n",
    "\n",
    "        return speaker_captions\n",
    "\n",
    "    def __call__(self, diarization, waveform):\n",
    "        # Step 1: Transcribe\n",
    "        transcription = self.transcribe(waveform)\n",
    "        # Update transcription buffer\n",
    "        self._buffer += transcription[\"text\"]\n",
    "        # The audio may not be the beginning of the conversation\n",
    "        time_shift = waveform.sliding_window.start\n",
    "        # Step 2: Assign speakers\n",
    "        speaker_transcriptions = self.identify_speakers(transcription, diarization, time_shift)\n",
    "        return speaker_transcriptions\n",
    "\n",
    "\n",
    "# Suppress whisper-timestamped warnings for a clean output\n",
    "logging.getLogger(\"whisper_timestamped\").setLevel(logging.ERROR)\n",
    "\n",
    "# If you have a GPU, you can also set device=torch.device(\"cuda\")\n",
    "config = PipelineConfig(\n",
    "    duration=5,\n",
    "    step=0.5,\n",
    "    latency=\"min\",\n",
    "    tau_active=0.5,\n",
    "    rho_update=0.1,\n",
    "    delta_new=0.57\n",
    ")\n",
    "dia = OnlineSpeakerDiarization(config)\n",
    "source = MicrophoneAudioSource(config.sample_rate)\n",
    "\n",
    "# If you have a GPU, you can also set device=\"cuda\"\n",
    "asr = WhisperTranscriber(model=\"small\")\n",
    "\n",
    "# Split the stream into 2s chunks for transcription\n",
    "transcription_duration = 2\n",
    "# Apply models in batches for better efficiency\n",
    "batch_size = int(transcription_duration // config.step)\n",
    "\n",
    "# Chain of operations to apply on the stream of microphone audio\n",
    "source.stream.pipe(\n",
    "    # Format audio stream to sliding windows of 5s with a step of 500ms\n",
    "    dops.rearrange_audio_stream(\n",
    "        config.duration, config.step, config.sample_rate\n",
    "    ),\n",
    "    # Wait until a batch is full\n",
    "    # The output is a list of audio chunks\n",
    "    ops.buffer_with_count(count=batch_size),\n",
    "    # Obtain diarization prediction\n",
    "    # The output is a list of pairs `(diarization, audio chunk)`\n",
    "    ops.map(dia),\n",
    "    # Concatenate 500ms predictions/chunks to form a single 2s chunk\n",
    "    ops.map(concat),\n",
    "    # Ignore this chunk if it does not contain speech\n",
    "    ops.filter(lambda ann_wav: ann_wav[0].get_timeline().duration() > 0),\n",
    "    # Obtain speaker-aware transcriptions\n",
    "    # The output is a list of pairs `(speaker: int, caption: str)`\n",
    "    ops.starmap(asr),\n",
    "    # Color transcriptions according to the speaker\n",
    "    # The output is plain text with color references for rich\n",
    "    ops.map(colorize_transcription),\n",
    ").subscribe(\n",
    "    on_next=rich.print,  # print colored text\n",
    "    on_error=lambda _: traceback.print_exc()  # print stacktrace if error\n",
    ")\n",
    "\n",
    "print(\"Listening...\")\n",
    "source.read()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
